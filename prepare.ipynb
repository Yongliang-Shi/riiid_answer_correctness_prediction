{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: to prepare the dataset for exploration and modeling\n",
    "- data source: a sample of users' records from Riiid database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Warnings \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Monitor memory use and time\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import the prepare.py\n",
    "import prepare\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momery usage:  43.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((411517, 18), (50842, 18), (52868, 18))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in data from local csv files\n",
    "df_train = pd.read_csv('data_2000users/train.csv')\n",
    "df_validate = pd.read_csv('data_2000users/validate.csv')\n",
    "df_test = pd.read_csv('data_2000users/test.csv')\n",
    "\n",
    "# Print how much memory has been used\n",
    "print(\"Momery usage: \", psutil.virtual_memory().percent)\n",
    "\n",
    "# Print the shapes of the loaded files\n",
    "df_train.shape, df_validate.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momoery usage:  43.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13523, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the questions_with_tag_counts.csv\n",
    "df_ques = pd.read_csv('questions_with_tag_counts.csv', index_col=0)\n",
    "# df_lects = pd.read_csv('lectures_with_part_name.csv', index_col=0)\n",
    "\n",
    "# Print how much memory has been used\n",
    "print(\"Momoery usage: \", psutil.virtual_memory().percent)\n",
    "\n",
    "# Print its shape\n",
    "df_ques.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns from Kaggle questions.csv and lectures.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momoery usage:  43.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((411517, 9), (50842, 9), (52868, 9))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the columns need to be dropped\n",
    "cols = ['lecture_id', 'tag', 'lecture_part', 'type_of', 'question_id',\n",
    "        'bundle_id', 'correct_answer', 'question_part', 'tags']\n",
    "\n",
    "# Drop the columns in train, validate, and test dataset\n",
    "df_train = df_train.drop(columns = cols)\n",
    "df_validate = df_validate.drop(columns = cols)\n",
    "df_test = df_test.drop(columns = cols)\n",
    "\n",
    "# Print how much memory has been used\n",
    "print(\"Momoery usage: \", psutil.virtual_memory().percent)\n",
    "\n",
    "# Inspect the shapes of the dataframes\n",
    "df_train.shape, df_validate.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features:\n",
    "- user_acc_mean\n",
    "- user_lectures_running_total\n",
    "- avg_user_q_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momoery usage:  43.3\n",
      "CPU times: user 2.55 s, sys: 198 ms, total: 2.75 s\n",
      "Wall time: 2.79 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((411517, 13), (50842, 12), (52868, 12))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate how much time it takes to run the cell\n",
    "\n",
    "# Use the helper function in prepare.py to add the new features\n",
    "train = prepare.sam_train_features(df_train)\n",
    "validate = prepare.sam_valtest_features(train, df_validate)\n",
    "test = prepare.sam_valtest_features(train, df_test)\n",
    "\n",
    "# Print how much memory has been used\n",
    "print(\"Momoery usage: \", psutil.virtual_memory().percent)\n",
    "\n",
    "# Print the shapes of the dataframes\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop user ids to save memory\n",
    "\n",
    "# train.drop(columns='user_id', inplace=True)\n",
    "# validate.drop(columns='user_id', inplace=True)\n",
    "# test.drop(columns='user_id', inplace=True)\n",
    "\n",
    "# train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle nulls and the np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>content_type_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>user_answer</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "      <th>user_acc_mean</th>\n",
       "      <th>user_lectures_running_total</th>\n",
       "      <th>q_time</th>\n",
       "      <th>avg_user_q_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1864702</td>\n",
       "      <td>5720</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.630049</td>\n",
       "      <td>0</td>\n",
       "      <td>45951.0</td>\n",
       "      <td>11917302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45951</td>\n",
       "      <td>1864702</td>\n",
       "      <td>5204</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>inf</td>\n",
       "      <td>False</td>\n",
       "      <td>0.630049</td>\n",
       "      <td>0</td>\n",
       "      <td>28391.0</td>\n",
       "      <td>11917302.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  user_id  content_id  content_type_id  task_container_id  \\\n",
       "0          0  1864702        5720                0                  0   \n",
       "1      45951  1864702        5204                0                  1   \n",
       "\n",
       "   user_answer  answered_correctly  prior_question_elapsed_time  \\\n",
       "0            1                   1                          NaN   \n",
       "1            1                   0                          inf   \n",
       "\n",
       "  prior_question_had_explanation  user_acc_mean  user_lectures_running_total  \\\n",
       "0                            NaN       0.630049                            0   \n",
       "1                          False       0.630049                            0   \n",
       "\n",
       "    q_time  avg_user_q_time  \n",
       "0  45951.0       11917302.0  \n",
       "1  28391.0       11917302.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the train dataset before\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>content_type_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>user_answer</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "      <th>user_acc_mean</th>\n",
       "      <th>user_lectures_running_total</th>\n",
       "      <th>q_time</th>\n",
       "      <th>avg_user_q_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1864702</td>\n",
       "      <td>5720</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.630049</td>\n",
       "      <td>0</td>\n",
       "      <td>45951.0</td>\n",
       "      <td>11917302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45951</td>\n",
       "      <td>1864702</td>\n",
       "      <td>5204</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.630049</td>\n",
       "      <td>0</td>\n",
       "      <td>28391.0</td>\n",
       "      <td>11917302.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  user_id  content_id  content_type_id  task_container_id  \\\n",
       "0          0  1864702        5720                0                  0   \n",
       "1      45951  1864702        5204                0                  1   \n",
       "\n",
       "   user_answer  answered_correctly  prior_question_elapsed_time  \\\n",
       "0            1                   1                          0.0   \n",
       "1            1                   0                          0.0   \n",
       "\n",
       "   prior_question_had_explanation  user_acc_mean  user_lectures_running_total  \\\n",
       "0                           False       0.630049                            0   \n",
       "1                           False       0.630049                            0   \n",
       "\n",
       "    q_time  avg_user_q_time  \n",
       "0  45951.0       11917302.0  \n",
       "1  28391.0       11917302.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the helper function in prepare.py to handle the nulls\n",
    "train = prepare.handle_null(train)\n",
    "validate = prepare.handle_null(validate)\n",
    "test = prepare.handle_null(test)\n",
    "    \n",
    "# Use the helper function in prepare.py to handle the np.inf\n",
    "train = prepare.handle_inf(train)\n",
    "validate = prepare.handle_inf(validate)\n",
    "test = prepare.handle_inf(test)\n",
    "\n",
    "# Inspect the train after\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to separate the lecture rows and question rows\n",
    "\n",
    "# def seperate_rows(df):\n",
    "#     '''\n",
    "#     separate the lecture rows and question rows\n",
    "#     '''\n",
    "#     mask_question = (df['answered_correctly'] != -1)\n",
    "#     mask_lecture = (df['answered_correctly'] == -1)\n",
    "#     df_question = df[mask_question]\n",
    "#     df_lecture = df[mask_lecture]\n",
    "#     return df_question, df_lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply the function on train, validate, and test\n",
    "\n",
    "# train, train_lects = seperate_rows(train)\n",
    "# validate, validate_lects = seperate_rows(validate)\n",
    "# test, test_lects = seperate_rows(test)\n",
    "\n",
    "# train.shape, train_lects.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the lecture rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411517, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the shape of the train before the process\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(403377, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the helper function in prepare.py to drop the lecture rows in train/validate/test\n",
    "train = prepare.drop_lecture_rows(train)\n",
    "validate = prepare.drop_lecture_rows(validate)\n",
    "test = prepare.drop_lecture_rows(test)\n",
    "\n",
    "# Print the shape of the train after\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge: \n",
    "- train, validate and test with df_ques\n",
    "- train_lects with df_lects: not done yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momoery usage:  44.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((403377, 19), (49945, 18), (51971, 18))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge train/validate/test with df_ques\n",
    "\n",
    "train = train.merge(df_ques, how='left', left_on='content_id', right_on='question_id')\n",
    "validate = validate.merge(df_ques, how='left', left_on='content_id', right_on='question_id')\n",
    "test = test.merge(df_ques, how='left', left_on='content_id', right_on='question_id')\n",
    "\n",
    "# Print how much memory has been used\n",
    "print(\"Momoery usage: \", psutil.virtual_memory().percent)\n",
    "\n",
    "# Print the shapes of the dataframes\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>content_type_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>user_answer</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "      <th>user_acc_mean</th>\n",
       "      <th>user_lectures_running_total</th>\n",
       "      <th>q_time</th>\n",
       "      <th>avg_user_q_time</th>\n",
       "      <th>question_id</th>\n",
       "      <th>bundle_id</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>part</th>\n",
       "      <th>tags</th>\n",
       "      <th>tag_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1864702</td>\n",
       "      <td>5720</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.630049</td>\n",
       "      <td>0</td>\n",
       "      <td>45951.0</td>\n",
       "      <td>11917302.0</td>\n",
       "      <td>5720</td>\n",
       "      <td>5720</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  user_id  content_id  content_type_id  task_container_id  \\\n",
       "0          0  1864702        5720                0                  0   \n",
       "\n",
       "   user_answer  answered_correctly  prior_question_elapsed_time  \\\n",
       "0            1                   1                          0.0   \n",
       "\n",
       "   prior_question_had_explanation  user_acc_mean  user_lectures_running_total  \\\n",
       "0                           False       0.630049                            0   \n",
       "\n",
       "    q_time  avg_user_q_time  question_id  bundle_id  correct_answer  part  \\\n",
       "0  45951.0       11917302.0         5720       5720               1     5   \n",
       "\n",
       "  tags  tag_count  \n",
       "0  115          1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check to make sure the merge is successful\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Columns to drop**\n",
    "- timestamp: used to compute avg time each user to answer 1 question\n",
    "- content_id (question_id): after drop the lecture rows, used to compute average question mean\n",
    "- content_type_id: **drop**\n",
    "- task_container_id: used to compute avereage task mean\n",
    "- user_answer: **drop**\n",
    "- answered_correctly: target varibale\n",
    "- prior_question_elapsed_time: **drop**\n",
    "- prior_question_had_explanation: used for question_had_explanation\n",
    "- correct_answer: **drop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the redundant columns to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momoery usage:  44.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((403377, 15), (49945, 14), (51971, 14))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the redundant column to save memory\n",
    "\n",
    "train.drop(columns=['content_type_id', 'user_answer', \n",
    "                    'prior_question_elapsed_time', 'correct_answer'], inplace=True)\n",
    "validate.drop(columns=['content_type_id', 'user_answer', \n",
    "                       'prior_question_elapsed_time', 'correct_answer'], inplace=True)\n",
    "test.drop(columns=['content_type_id', 'user_answer', \n",
    "                   'prior_question_elapsed_time', 'correct_answer'], inplace=True)\n",
    "\n",
    "# Print how much memory has been used\n",
    "print(\"Momoery usage: \", psutil.virtual_memory().percent)\n",
    "\n",
    "# Print the shapes of the dataframes\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "      <th>user_acc_mean</th>\n",
       "      <th>user_lectures_running_total</th>\n",
       "      <th>q_time</th>\n",
       "      <th>avg_user_q_time</th>\n",
       "      <th>question_id</th>\n",
       "      <th>bundle_id</th>\n",
       "      <th>part</th>\n",
       "      <th>tags</th>\n",
       "      <th>tag_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1864702</td>\n",
       "      <td>5720</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.630049</td>\n",
       "      <td>0</td>\n",
       "      <td>45951.0</td>\n",
       "      <td>11917302.0</td>\n",
       "      <td>5720</td>\n",
       "      <td>5720</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  user_id  content_id  task_container_id  answered_correctly  \\\n",
       "0          0  1864702        5720                  0                   1   \n",
       "\n",
       "   prior_question_had_explanation  user_acc_mean  user_lectures_running_total  \\\n",
       "0                           False       0.630049                            0   \n",
       "\n",
       "    q_time  avg_user_q_time  question_id  bundle_id  part tags  tag_count  \n",
       "0  45951.0       11917302.0         5720       5720     5  115          1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check if the columns above has been removed\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features:\n",
    "- part accuracy\n",
    "- bundle accuray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(403377, 15)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_bundle_features(train, validate, test):\n",
    "    \n",
    "    # Calculate the average accuracy for each unique bundle id\n",
    "    bundle_accuracy = train.groupby(['bundle_id'])['answered_correctly'].mean().round(2).to_frame().reset_index()\n",
    "    bundle_accuracy.columns = ['bundle_id', 'mean_bundle_accuracy']\n",
    "    \n",
    "    # Add bundle mean accuracy as a feature to train, validate, and test\n",
    "    merged_train = train.merge(bundle_accuracy, left_on='bundle_id', right_on='bundle_id', how='left')\n",
    "    merged_validate = validate.merge(bundle_accuracy, left_on='bundle_id', right_on='bundle_id', how='left')\n",
    "    merged_test = test.merge(bundle_accuracy, left_on='bundle_id', right_on='bundle_id', how='left')\n",
    "    \n",
    "    # Calculate the average part accuracy\n",
    "    tag_accuracy = train.groupby(['part'])['answered_correctly'].agg(['mean']).round(2).reset_index()\n",
    "    tag_accuracy.columns = ['part', 'mean_part_accuracy']\n",
    "    \n",
    "    # Add average part accuracy\n",
    "    train_df = merged_train.merge(tag_accuracy, left_on='part', right_on='part')\n",
    "    validate_df = merged_validate.merge(tag_accuracy, left_on='part', right_on='part')\n",
    "    test_df = merged_test.merge(tag_accuracy, left_on='part', right_on='part')\n",
    "    \n",
    "    # Calculate the mean container accuracy for each part\n",
    "    tag_bundles = train.groupby(['question_id', 'task_container_id', 'part'])['answered_correctly'].mean().round(2).reset_index()\n",
    "    tag_bundles.rename(columns={'answered_correctly': 'mean_container_part_accuracy'}, inplace=True)\n",
    "#     tag_bundles.drop(columns='question_id', inplace=True)\n",
    "    \n",
    "    # Add mean container part accuracy\n",
    "    train_set = train_df.merge(tag_bundles, how='left', \n",
    "                               left_on=['task_container_id', 'part', 'question_id'], \n",
    "                               right_on=['task_container_id', 'part', 'question_id'])\n",
    "    \n",
    "    validate_set = validate_df.merge(tag_bundles, how='left', \n",
    "                                     left_on=['task_container_id', 'part', 'question_id'], \n",
    "                                     right_on=['task_container_id', 'part', 'question_id'])\n",
    "    \n",
    "    test_set = test_df.merge(tag_bundles, how='left', \n",
    "                             left_on=['task_container_id', 'part', 'question_id'], \n",
    "                             right_on=['task_container_id', 'part', 'question_id'])\n",
    "\n",
    "    \n",
    "    return train_set, validate_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momoery usage:  48.1\n",
      "CPU times: user 598 ms, sys: 150 ms, total: 748 ms\n",
      "Wall time: 762 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((403377, 18), (49945, 17), (51971, 17))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Compute the time to process the cell\n",
    "\n",
    "# Use the function defined above to add the new features\n",
    "train, validate, test = part_bundle_features(train, validate, test)\n",
    "\n",
    "# Print how much memory has been used\n",
    "print(\"Momoery usage: \", psutil.virtual_memory().percent)\n",
    "\n",
    "# Print the shapes of the dataframes\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "      <th>user_acc_mean</th>\n",
       "      <th>user_lectures_running_total</th>\n",
       "      <th>q_time</th>\n",
       "      <th>avg_user_q_time</th>\n",
       "      <th>question_id</th>\n",
       "      <th>bundle_id</th>\n",
       "      <th>part</th>\n",
       "      <th>tags</th>\n",
       "      <th>tag_count</th>\n",
       "      <th>mean_bundle_accuracy</th>\n",
       "      <th>mean_part_accuracy</th>\n",
       "      <th>mean_container_part_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1864702</td>\n",
       "      <td>5720</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.630049</td>\n",
       "      <td>0</td>\n",
       "      <td>45951.0</td>\n",
       "      <td>11917302.0</td>\n",
       "      <td>5720</td>\n",
       "      <td>5720</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  user_id  content_id  task_container_id  answered_correctly  \\\n",
       "0          0  1864702        5720                  0                   1   \n",
       "\n",
       "   prior_question_had_explanation  user_acc_mean  user_lectures_running_total  \\\n",
       "0                           False       0.630049                            0   \n",
       "\n",
       "    q_time  avg_user_q_time  question_id  bundle_id  part tags  tag_count  \\\n",
       "0  45951.0       11917302.0         5720       5720     5  115          1   \n",
       "\n",
       "   mean_bundle_accuracy  mean_part_accuracy  mean_container_part_accuracy  \n",
       "0                  0.82                0.61                           1.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the new features have been added\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features\n",
    "- content accuracy\n",
    "- task accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momoery usage:  60.5\n",
      "CPU times: user 6.7 s, sys: 137 ms, total: 6.83 s\n",
      "Wall time: 6.88 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((403377, 19), (49945, 18), (51971, 18))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Compute the time to process the cell\n",
    "\n",
    "# Use the helper function in prepare.py to add the features\n",
    "train = prepare.merge_with_stats_train(train)\n",
    "validate = prepare.merge_with_stats_valortest(train, validate)\n",
    "test = prepare.merge_with_stats_valortest(train, test)\n",
    "\n",
    "# Print how much memory has been used\n",
    "print(\"Momoery usage: \", psutil.virtual_memory().percent)\n",
    "\n",
    "# Print the shapes of the dataframes\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "      <th>user_acc_mean</th>\n",
       "      <th>user_lectures_running_total</th>\n",
       "      <th>q_time</th>\n",
       "      <th>avg_user_q_time</th>\n",
       "      <th>question_id</th>\n",
       "      <th>bundle_id</th>\n",
       "      <th>part</th>\n",
       "      <th>tags</th>\n",
       "      <th>tag_count</th>\n",
       "      <th>mean_bundle_accuracy</th>\n",
       "      <th>mean_part_accuracy</th>\n",
       "      <th>mean_content_accuracy</th>\n",
       "      <th>mean_task_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1864702</td>\n",
       "      <td>5720</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.630049</td>\n",
       "      <td>0</td>\n",
       "      <td>45951.0</td>\n",
       "      <td>11917302.0</td>\n",
       "      <td>5720</td>\n",
       "      <td>5720</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.682248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  user_id  content_id  task_container_id  answered_correctly  \\\n",
       "0          0  1864702        5720                  0                   1   \n",
       "\n",
       "   prior_question_had_explanation  user_acc_mean  user_lectures_running_total  \\\n",
       "0                           False       0.630049                            0   \n",
       "\n",
       "    q_time  avg_user_q_time  question_id  bundle_id  part tags  tag_count  \\\n",
       "0  45951.0       11917302.0         5720       5720     5  115          1   \n",
       "\n",
       "   mean_bundle_accuracy  mean_part_accuracy  mean_content_accuracy  \\\n",
       "0                  0.82                0.61               0.818182   \n",
       "\n",
       "   mean_task_accuracy  \n",
       "0            0.682248  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the result\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features:\n",
    "- mean tagcount accuracy\n",
    "- mean tags accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions to add the features to train/validate/test\n",
    "\n",
    "def mean_tagcount_accuracy(df):\n",
    "    tagcount_accuracy = df.groupby('tag_count').answered_correctly.mean().round(2).rename('mean_tagcount_accuracy')\n",
    "    return tagcount_accuracy\n",
    "\n",
    "def mean_tag_accuracy(df):\n",
    "    tags_accuracy = df.groupby('tags').answered_correctly.mean().round(2).rename('mean_tags_accuracy')\n",
    "    return tags_accuracy\n",
    "\n",
    "def tag_features(train, validate, test):\n",
    "    \n",
    "    tagcount_accuracy = mean_tagcount_accuracy(train)\n",
    "    tags_accuracy = mean_tag_accuracy(train)\n",
    "\n",
    "    train = train.merge(tagcount_accuracy, how='left', on='tag_count')\n",
    "    train = train.merge(tags_accuracy, how='left', on='tags')\n",
    "\n",
    "    validate = validate.merge(tagcount_accuracy, how='left', on='tag_count')\n",
    "    validate = validate.merge(tags_accuracy, how='left', on='tags')\n",
    "\n",
    "    test = test.merge(tagcount_accuracy, how='left', on='tag_count')\n",
    "    test = test.merge(tags_accuracy, how='left', on='tags')\n",
    "\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used the defined function to add the features\n",
    "train, validate, test = tag_features(train, validate, test)\n",
    "\n",
    "# Check the result\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill the Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                            0\n",
       "user_id                              0\n",
       "content_id                           0\n",
       "task_container_id                    0\n",
       "answered_correctly                   0\n",
       "prior_question_had_explanation       0\n",
       "user_acc_mean                        0\n",
       "user_lectures_running_total          0\n",
       "avg_user_q_time                      0\n",
       "question_id                          0\n",
       "bundle_id                            0\n",
       "part                                 0\n",
       "tags                                 0\n",
       "tag_count                            0\n",
       "mean_bundle_accuracy               206\n",
       "mean_part_accuracy                   0\n",
       "mean_content_accuracy              206\n",
       "mean_task_accuracy                1405\n",
       "mean_tagcount_accuracy               0\n",
       "mean_tags_accuracy                   4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the missing values in the validate dataset before filling the nulls\n",
    "validate.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                         0\n",
       "user_id                           0\n",
       "content_id                        0\n",
       "task_container_id                 0\n",
       "answered_correctly                0\n",
       "prior_question_had_explanation    0\n",
       "user_acc_mean                     0\n",
       "user_lectures_running_total       0\n",
       "avg_user_q_time                   0\n",
       "question_id                       0\n",
       "bundle_id                         0\n",
       "part                              0\n",
       "tags                              0\n",
       "tag_count                         0\n",
       "mean_bundle_accuracy              0\n",
       "mean_part_accuracy                0\n",
       "mean_content_accuracy             0\n",
       "mean_task_accuracy                0\n",
       "mean_tagcount_accuracy            0\n",
       "mean_tags_accuracy                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the helper function in prepare.py to fill the nulls\n",
    "validate = prepare.fill_nulls(validate)\n",
    "test = prepare.fill_nulls(test)\n",
    "\n",
    "# Check the result\n",
    "validate.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift prior question had explanation to current question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift prior question had explanation to current question\n",
    "\n",
    "train.prior_question_had_explanation = train.prior_question_had_explanation.shift(-1)\n",
    "validate.prior_question_had_explanation = validate.prior_question_had_explanation.shift(-1)\n",
    "test.prior_question_had_explanation = test.prior_question_had_explanation.shift(-1)\n",
    "\n",
    "train = train.rename(columns={\"prior_question_had_explanation\": \"question_had_explanation\"})\n",
    "validate = validate.rename(columns={\"prior_question_had_explanation\": \"question_had_explanation\"})\n",
    "test = test.rename(columns={\"prior_question_had_explanation\": \"question_had_explanation\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv('train_exploration.csv')\n",
    "# validate.to_csv('validate_exploration.csv')\n",
    "# test.to_csv('test_exploration.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the q_time in the train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>question_had_explanation</th>\n",
       "      <th>user_acc_mean</th>\n",
       "      <th>user_lectures_running_total</th>\n",
       "      <th>avg_user_q_time</th>\n",
       "      <th>question_id</th>\n",
       "      <th>bundle_id</th>\n",
       "      <th>part</th>\n",
       "      <th>tags</th>\n",
       "      <th>tag_count</th>\n",
       "      <th>mean_bundle_accuracy</th>\n",
       "      <th>mean_part_accuracy</th>\n",
       "      <th>mean_content_accuracy</th>\n",
       "      <th>mean_task_accuracy</th>\n",
       "      <th>mean_tagcount_accuracy</th>\n",
       "      <th>mean_tags_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1864702</td>\n",
       "      <td>5720</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.630049</td>\n",
       "      <td>0</td>\n",
       "      <td>11917302.0</td>\n",
       "      <td>5720</td>\n",
       "      <td>5720</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.682248</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  user_id  content_id  task_container_id  answered_correctly  \\\n",
       "0          0  1864702        5720                  0                   1   \n",
       "\n",
       "  question_had_explanation  user_acc_mean  user_lectures_running_total  \\\n",
       "0                    False       0.630049                            0   \n",
       "\n",
       "   avg_user_q_time  question_id  bundle_id  part tags  tag_count  \\\n",
       "0       11917302.0         5720       5720     5  115          1   \n",
       "\n",
       "   mean_bundle_accuracy  mean_part_accuracy  mean_content_accuracy  \\\n",
       "0                  0.82                0.61               0.818182   \n",
       "\n",
       "   mean_task_accuracy  mean_tagcount_accuracy  mean_tags_accuracy  \n",
       "0            0.682248                    0.62                0.79  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop column q_time in the train\n",
    "train_s = train.drop(columns='q_time')\n",
    "train_s.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns not needed for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to drop the unnecessary columns for modeling\n",
    "\n",
    "def drop_columns(df):\n",
    "    \"\"\"\n",
    "    Accepts df and drops various columns that are not needed for modeling.\n",
    "    \"\"\"\n",
    "    cols = ['timestamp', 'user_id', 'content_id', 'task_container_id',\n",
    "            'question_id', 'bundle_id', 'part', 'tags', 'tag_count']\n",
    "    df.drop(columns=cols, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((403377, 20), (49945, 20), (51971, 20))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the shapes of the dataframes before dropping\n",
    "train_s.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((403377, 11), (49945, 11), (51971, 11))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the defined function to drop the columns\n",
    "train_s = drop_columns(train_s)\n",
    "validate_s = drop_columns(validate)\n",
    "test_s = drop_columns(test)\n",
    "\n",
    "# Check the result\n",
    "train_s.shape, validate_s.shape, test_s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert boolean to num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to convert the boolean values to the numeric\n",
    "\n",
    "def boolean_to_num(df):\n",
    "    \"\"\"\n",
    "    Accepts df. Converts True and False values into 1's and 0's resepectively, within the \n",
    "    question_had_explanation column.\n",
    "    \"\"\"\n",
    "    df = df.fillna(False)\n",
    "    m = df.question_had_explanation.apply(lambda i: 1 if i == True else 0)\n",
    "    df.question_had_explanation = m\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the defined function to do the conversion\n",
    "\n",
    "train_s = boolean_to_num(train_s)\n",
    "validate_s = boolean_to_num(validate_s)\n",
    "test_s = boolean_to_num(test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answered_correctly               int64\n",
       "question_had_explanation         int64\n",
       "user_acc_mean                  float64\n",
       "user_lectures_running_total      int64\n",
       "avg_user_q_time                float64\n",
       "mean_bundle_accuracy           float64\n",
       "mean_part_accuracy             float64\n",
       "mean_content_accuracy          float64\n",
       "mean_task_accuracy             float64\n",
       "mean_tagcount_accuracy         float64\n",
       "mean_tags_accuracy             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the result\n",
    "test_s.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fucntion to scale the datasets\n",
    "\n",
    "def scale(train, validate, test, columns_to_scale):\n",
    "    '''\n",
    "    Accepts train, validate, test and list of columns to scale. Scales listed columns.\n",
    "    '''\n",
    "    new_column_names = [c + '_scaled' for c in columns_to_scale]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler = scaler.fit(train[columns_to_scale])\n",
    "\n",
    "    train = pd.concat([\n",
    "        train,\n",
    "        pd.DataFrame(scaler.transform(train[columns_to_scale]), columns=new_column_names, index=train.index),\n",
    "    ], axis=1)\n",
    "\n",
    "    validate = pd.concat([\n",
    "        validate,\n",
    "        pd.DataFrame(scaler.transform(validate[columns_to_scale]), columns=new_column_names, index=validate.index),\n",
    "    ], axis=1)\n",
    "\n",
    "    test = pd.concat([\n",
    "        test,\n",
    "        pd.DataFrame(scaler.transform(test[columns_to_scale]), columns=new_column_names, index=test.index),\n",
    "    ], axis=1)\n",
    "    \n",
    "    train.drop(columns=columns_to_scale, inplace=True)\n",
    "    validate.drop(columns=columns_to_scale, inplace=True)\n",
    "    test.drop(columns=columns_to_scale, inplace=True)\n",
    "    \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns needs to be scaled\n",
    "columns_to_scale = ['user_lectures_running_total', 'avg_user_q_time']\n",
    "\n",
    "# Use the defined function to scaled the datasets\n",
    "train_s, validate_s, test_s = scale(train_s, validate_s, test_s, columns_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>question_had_explanation</th>\n",
       "      <th>user_acc_mean</th>\n",
       "      <th>mean_bundle_accuracy</th>\n",
       "      <th>mean_part_accuracy</th>\n",
       "      <th>mean_content_accuracy</th>\n",
       "      <th>mean_task_accuracy</th>\n",
       "      <th>mean_tagcount_accuracy</th>\n",
       "      <th>mean_tags_accuracy</th>\n",
       "      <th>user_lectures_running_total_scaled</th>\n",
       "      <th>avg_user_q_time_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.630049</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.682248</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answered_correctly  question_had_explanation  user_acc_mean  \\\n",
       "0                   1                         0       0.630049   \n",
       "\n",
       "   mean_bundle_accuracy  mean_part_accuracy  mean_content_accuracy  \\\n",
       "0                  0.82                0.61               0.818182   \n",
       "\n",
       "   mean_task_accuracy  mean_tagcount_accuracy  mean_tags_accuracy  \\\n",
       "0            0.682248                    0.62                0.79   \n",
       "\n",
       "   user_lectures_running_total_scaled  avg_user_q_time_scaled  \n",
       "0                                 0.0                0.001202  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check results\n",
    "train_s.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one function to complete all the preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.71 s, sys: 725 ms, total: 10.4 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute the time to process the cell\n",
    "\n",
    "# Test the fucntion to complete all the steps above\n",
    "train, validate, test, train_s, validate_s, test_s = prepare.prep_riiid(df_train, df_validate, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((403377, 21), (49945, 20), (51971, 20))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the shapes of the train/validate/test for exploratioin\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>question_had_explanation</th>\n",
       "      <th>user_acc_mean</th>\n",
       "      <th>user_lectures_running_total</th>\n",
       "      <th>q_time</th>\n",
       "      <th>avg_user_q_time</th>\n",
       "      <th>...</th>\n",
       "      <th>bundle_id</th>\n",
       "      <th>part</th>\n",
       "      <th>tags</th>\n",
       "      <th>tag_count</th>\n",
       "      <th>mean_bundle_accuracy</th>\n",
       "      <th>mean_part_accuracy</th>\n",
       "      <th>mean_content_accuracy</th>\n",
       "      <th>mean_task_accuracy</th>\n",
       "      <th>mean_tagcount_accuracy</th>\n",
       "      <th>mean_tags_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1864702</td>\n",
       "      <td>5720</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.630049</td>\n",
       "      <td>0</td>\n",
       "      <td>45951.0</td>\n",
       "      <td>11917302.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5720</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  user_id  content_id  task_container_id  answered_correctly  \\\n",
       "0          0  1864702        5720                  0                   1   \n",
       "\n",
       "  question_had_explanation  user_acc_mean  user_lectures_running_total  \\\n",
       "0                    False       0.630049                            0   \n",
       "\n",
       "    q_time  avg_user_q_time  ...  bundle_id  part  tags tag_count  \\\n",
       "0  45951.0       11917302.0  ...       5720     5   115         1   \n",
       "\n",
       "   mean_bundle_accuracy  mean_part_accuracy  mean_content_accuracy  \\\n",
       "0                  0.82                0.61                   0.82   \n",
       "\n",
       "   mean_task_accuracy  mean_tagcount_accuracy  mean_tags_accuracy  \n",
       "0                0.68                    0.62                0.79  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the columns of the train dataset\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((403377, 11), (49945, 11), (51971, 11))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the shapes of the scaled train/validate/test for modeling\n",
    "train_s.shape, validate_s.shape, test_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>question_had_explanation</th>\n",
       "      <th>user_acc_mean</th>\n",
       "      <th>mean_bundle_accuracy</th>\n",
       "      <th>mean_part_accuracy</th>\n",
       "      <th>mean_content_accuracy</th>\n",
       "      <th>mean_task_accuracy</th>\n",
       "      <th>mean_tagcount_accuracy</th>\n",
       "      <th>mean_tags_accuracy</th>\n",
       "      <th>user_lectures_running_total_scaled</th>\n",
       "      <th>avg_user_q_time_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.630049</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answered_correctly  question_had_explanation  user_acc_mean  \\\n",
       "0                   1                         0       0.630049   \n",
       "\n",
       "   mean_bundle_accuracy  mean_part_accuracy  mean_content_accuracy  \\\n",
       "0                  0.82                0.61                   0.82   \n",
       "\n",
       "   mean_task_accuracy  mean_tagcount_accuracy  mean_tags_accuracy  \\\n",
       "0                0.68                    0.62                0.79   \n",
       "\n",
       "   user_lectures_running_total_scaled  avg_user_q_time_scaled  \n",
       "0                                 0.0                0.001202  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the columns of the scaled train dataset\n",
    "train_s.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select K Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train, validate, test DFs that only include non-target variables\n",
    "X_train = train_s.drop(columns='answered_correctly')\n",
    "y_train = train_s['answered_correctly']\n",
    "\n",
    "X_validate = validate_s.drop(columns='answered_correctly')\n",
    "y_validate = validate_s['answered_correctly']\n",
    "\n",
    "X_test = test_s.drop(columns='answered_correctly')\n",
    "y_test = test_s['answered_correctly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to rank all the features\n",
    "\n",
    "def KBest_ranker(X, y, n):\n",
    "    '''\n",
    "    Returns the top n selected features with their scores based on the SelectKBest calss\n",
    "    Parameters: scaled predictors(X) in df, target(y) in df, the number of features to select(n)\n",
    "    '''\n",
    "\n",
    "    # parameters: f_regression stats test, give me 5 features\n",
    "    f_selector = SelectKBest(f_classif, k=n)\n",
    "\n",
    "    # Fit on X and y\n",
    "    f_selector.fit(X, y)\n",
    "\n",
    "    # Calculate the score for each feature \n",
    "    feature_score = f_selector.scores_.round(2)\n",
    "\n",
    "    # Put the features and their score in a dataframe\n",
    "    df_features = pd.DataFrame({'features': X.columns, \n",
    "                                'score': feature_score})\n",
    "\n",
    "    # Sort the features based on their score\n",
    "    df_features.sort_values(by=\"score\", ascending=False, inplace=True, ignore_index=True)\n",
    "\n",
    "    # Compute how many features in X\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Add a rank column\n",
    "    df_features['rank'] = range(1, m+1)\n",
    "    \n",
    "    return df_features[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean_content_accuracy</td>\n",
       "      <td>76690.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean_bundle_accuracy</td>\n",
       "      <td>55971.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_acc_mean</td>\n",
       "      <td>22419.10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mean_tags_accuracy</td>\n",
       "      <td>21599.32</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean_task_accuracy</td>\n",
       "      <td>15910.84</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>question_had_explanation</td>\n",
       "      <td>3954.75</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mean_part_accuracy</td>\n",
       "      <td>3397.75</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mean_tagcount_accuracy</td>\n",
       "      <td>2389.41</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>user_lectures_running_total_scaled</td>\n",
       "      <td>211.82</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>avg_user_q_time_scaled</td>\n",
       "      <td>116.95</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             features     score  rank\n",
       "0               mean_content_accuracy  76690.75     1\n",
       "1                mean_bundle_accuracy  55971.00     2\n",
       "2                       user_acc_mean  22419.10     3\n",
       "3                  mean_tags_accuracy  21599.32     4\n",
       "4                  mean_task_accuracy  15910.84     5\n",
       "5            question_had_explanation   3954.75     6\n",
       "6                  mean_part_accuracy   3397.75     7\n",
       "7              mean_tagcount_accuracy   2389.41     8\n",
       "8  user_lectures_running_total_scaled    211.82     9\n",
       "9              avg_user_q_time_scaled    116.95    10"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the defined function to rank all the features in the scaled train dataset\n",
    "df = KBest_ranker(X_train, y_train, 10)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
